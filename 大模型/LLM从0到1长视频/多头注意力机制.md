# 多头注意力机制

两个矩阵相乘，其实是在求两个矩阵之间的相似度

![结构笔记图](https://github.com/Dyi11/StudyNotes/blob/main/%E5%A4%A7%E6%A8%A1%E5%9E%8B/LLM%E4%BB%8E0%E5%88%B01%E9%95%BF%E8%A7%86%E9%A2%91/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20251106214326.jpg?raw=true)

如果在多头注意力模块前面再加个LayerNorm，模型会效果更好，收敛速度更快
