用**tiktoken**来把训练大模型的文本给tokenizen化，跟普通的数据集训练一样，也要进行划分，划分成训练的数据和验证的数据

在大语言模型训练中，**以下模块的参数会在训练过程中被调整**：

### 1. 注意力机制相关参数
- **Q、K、V 投影矩阵**：这是多头注意力机制的核心参数。模型通过调整这些矩阵的权重，学习如何计算不同token之间的注意力权重，从而捕捉文本中的语义关联（比如让“猫”和“狗”的注意力权重体现出它们在语义上的接近性）。
- **输出（O）投影矩阵**：用于将多头注意力的结果进行整合，其参数也会在训练中更新。


### 2. 嵌入层（Embedding）参数
嵌入层的参数**会跟着训练变化**。它的作用是将离散的token索引（如单词编号）映射为连续的向量表示。在训练中，这些向量会不断调整，使得语义相近的token在向量空间中更接近（例如“国王”和“王后”的嵌入向量会逐渐形成符合语义的关联）。嵌入层参数是模型理解语言语义的“地基”，会与模型其他部分的参数一起被端到端地训练优化。


### 3. 其他关键参数
- **前馈网络（FFN）的权重矩阵**：用于对注意力输出进行进一步的特征变换，提升模型的表达能力。
- **层归一化（LayerNorm）的缩放和偏移参数**：用于稳定训练过程，加速模型收敛。
- **位置编码相关参数（若为可训练的位置编码）**：帮助模型感知token在序列中的位置信息，其参数也会在训练中更新。


简言之，大语言模型训练是对**嵌入层、注意力机制（Q/K/V/O）、前馈网络、层归一化等所有可学习参数的全局优化**，这些参数共同协作，使模型逐渐具备理解和生成语言的能力。

在 AI 模型微调领域，**LoRA** 是 “Low-Rank Adaptation” 的缩写，是一种高效微调大语言模型（LLM）或图像生成模型（如 Stable Diffusion）的技术。

核心原理：
大模型的参数规模通常达数十亿甚至千亿级，全量微调需消耗大量计算资源。**LoRA 通过在模型的关键层（如 Transformer 的注意力层）中插入低秩矩阵，仅训练这些新增的低秩矩阵参数，而非全量模型参数，从而大幅降低计算成本和存储开销**。

核心优势：
高效节能：训练参数仅为全量微调的千分之一甚至万分之一，普通 GPU 即可支持；
灵活组合：不同任务的 LoRA 权重可独立保存，按需加载（如给同一模型切换 “翻译”“写作”“绘画风格” 等不同能力）；
效果稳定：在多数任务中能达到接近全量微调的性能。
