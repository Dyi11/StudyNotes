用**tiktoken**来把训练大模型的文本给tokenizen化，跟普通的数据集训练一样，也要进行划分，划分成训练的数据和验证的数据

在大语言模型训练中，**以下模块的参数会在训练过程中被调整**：

### 1. 注意力机制相关参数
- **Q、K、V 投影矩阵**：这是多头注意力机制的核心参数。模型通过调整这些矩阵的权重，学习如何计算不同token之间的注意力权重，从而捕捉文本中的语义关联（比如让“猫”和“狗”的注意力权重体现出它们在语义上的接近性）。
- **输出（O）投影矩阵**：用于将多头注意力的结果进行整合，其参数也会在训练中更新。


### 2. 嵌入层（Embedding）参数
嵌入层的参数**会跟着训练变化**。它的作用是将离散的token索引（如单词编号）映射为连续的向量表示。在训练中，这些向量会不断调整，使得语义相近的token在向量空间中更接近（例如“国王”和“王后”的嵌入向量会逐渐形成符合语义的关联）。嵌入层参数是模型理解语言语义的“地基”，会与模型其他部分的参数一起被端到端地训练优化。


### 3. 其他关键参数
- **前馈网络（FFN）的权重矩阵**：用于对注意力输出进行进一步的特征变换，提升模型的表达能力。
- **层归一化（LayerNorm）的缩放和偏移参数**：用于稳定训练过程，加速模型收敛。
- **位置编码相关参数（若为可训练的位置编码）**：帮助模型感知token在序列中的位置信息，其参数也会在训练中更新。


简言之，大语言模型训练是对**嵌入层、注意力机制（Q/K/V/O）、前馈网络、层归一化等所有可学习参数的全局优化**，这些参数共同协作，使模型逐渐具备理解和生成语言的能力。
