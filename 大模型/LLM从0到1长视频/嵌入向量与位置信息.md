# Embedding

一句话拆成很多个单词，某一个单词也能拆成词根，总之是通过不断的拆分来来化简为更小的单元。

无数个这种单元构成了字典（词表），在机器中有特定的编码跟他一一对应。

一个词在不同语境下有不同的词性，也有不同的含义或者引申意义，所以有更多的维度，就能更好的学习理解这个词的意思。

batch类比句子的数量，token是每句话的单词个数。

比如：给了4句话输入进模型（inputs），一句话有16个单词，然后设置的是64维。那imput embedding matrix 的 shape（batch_size,context_length,d_model）为 [4,16,64]。16行64列，16行是一句话横着16个词转置变得列装，最后跟64维构成的16x64的状态。

# position encoding
Transformer 的位置编码公式选择正弦余弦，本质是用数学性质解决 “相对位置感知” 和 “长序列泛化” 这两个核心问题。它通过三角函数的周期性、对称性和组合性，让模型既能区分不同位置，又能学到位置间的相对关系，同时保证了扩展性和数值稳定性。





